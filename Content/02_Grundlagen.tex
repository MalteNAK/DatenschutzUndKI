\section{Grundlagen: Autorschaft und Künstliche Intelligenz}
\subsection{Historischer und philosophischer Begriff der Autorschaft}
Heutzutage, sowie seit der Neuzeit und insbesondere im Zuge der Aufklärung und der, im späten 18. Jahrhundert aufkommenden romantischen Genieästhetik, beherrscht die Rolle des Autors den Diskurs von fast jedem erdachten Werk (vgl. Barthes, \textit{Der Tod des Autors}). Im Folgenden soll das Autorschaftsverständnis untersucht werden, indem auf die Produktion eines kreativen Werks eingegangen wird. Zum Einen durch ein schöpferes Individuum oder durch die Kreation in kollektiver Form. Zudem werden die damit verbundenen Konzepte von Originalität, Inspiration und geistiges Eigentum behandelt, um den Einfluss von KI auf das Thema Autorschaft zu diskutieren.
\subsubsection{Das schöpferische Individuum}
Künstliche Intelligenz stört die Diskussionen rund um Autorschaft, indem die Fragen, wer ein Werk geschaffen hat und wie es geschaffen wurde, neu betrachtet werden müssen. Nach Immanuel Kants \textit{Kritik der Urteilskraft} entwickelt sich ein Verständnis von ästhetischer Autorschaft, welche untrennbar mit dem schöpferischen Individuum verbunden ist. Ästhetische Autorschaft setzt ein intentionales und verantwortbares Handeln voraus. Somit beginnt Autorschaft nicht nur wenn ein Werk entsteht, sondern diesem Werk ein Ausdruck eines Subjekts verliehen werden kann. Ein zentrales Konzept, welches Kant in diesem Prozess beschreibt, ist das Genie. Es sei ein natürliches Talent "durch welches die Natur der Kunst die Regel gibt". Demnach wird dem Schöpfer eines Werks von Kant mehr vorrausgesetzt als bloße Regelanwendung oder Nachahmung, sondern eine originäre schöpferische Leistung. Ein solche Leistung lässt sich niht vollständig rationalisieren oder in explizite Anweisungen überführen. Des Weiteren muss jenes Genie in der Lage sein, mithilfe von Einbildungskraft und Verstand, neue ästhetische Gedanken hervorzubringen. 

\subsubsection{Kollektive Kreativität}
Während Kant Autorschaft stark an das Individuum bindet, problematisieren Michel Foucault und Roland Barthes diese Vorstellung grundlegend. Beide lösen die Idee des Autors als originäre Quelle von Bedeutung auf, wenn auch mit unterschiedlichen Akzenten.\par

Barthes argumentiert, dass ein Text nicht als Ausdruck innerer Intentionalität des Autors zu verstehen sei, sondern als „Gewebe aus Zitaten“, das aus kulturellen, sprachlichen und historischen Bezügen besteht. Bedeutung entsteht demnach nicht im Akt des Schreibens, sondern im Lesen und im jeweiligen Kontext der Rezeption. Der Autor verliert seine privilegierte Stellung als Sinnstifter.\par

Foucault geht einen Schritt weiter, indem er den Autor nicht als natürliche Person, sondern als Funktion des Diskurses beschreibt. Die Autorfunktion erfüllt gesellschaftliche Aufgaben wie Zuschreibung, Verantwortungszuteilung, Kanonisierung und Kontrolle von Diskursen. Autorschaft entsteht dort, wo Zurechenbarkeit notwendig ist, nicht dort, wo kreative Produktion faktisch stattfindet.\par

Diese Perspektiven machen deutlich, dass kreative Werke stets in kollektiven Zusammenhängen entstehen: Sie greifen auf bestehende Diskurse zurück, reproduzieren kulturelle Muster und werden erst im sozialen Raum mit Bedeutung versehen. Kreativität ist somit nie rein individuell, sondern strukturell eingebettet.\par

Für KI bedeutet dies jedoch keine Aufwertung zur Autorschaft. Zwar operieren KI-Systeme paradigmatisch kollektiv, gespeist aus großen Datenmengen kultureller Artefakte, doch fehlt ihnen der Selbstbezug, der für die Autorfunktion zentral ist. KI kann weder Verantwortung tragen noch als diskursives Subjekt auftreten. Autorschaft bleibt daher eine menschliche Zuschreibungspraxis, auch wenn die Produktion selbst zunehmend kollektiv und technisch vermittelt ist.\par

\subsubsection{Originalität, Inspiration und geistiges Eigentum}
Die Konzepte Originalität und Inspiration sind historisch eng mit dem Ideal des schöpferischen Individuums verbunden. Bei Kant ist Originalität ein zentrales Merkmal des Genies: Ein Werk gilt als originell, wenn es nicht bloß bestehende Regeln reproduziert, sondern neue hervorbringt. Inspiration ist dabei kein mystischer Akt, sondern Ausdruck freier schöpferischer Tätigkeit.\par

Im Kontext Künstlicher Intelligenz geraten diese Begriffe unter Druck. KI-Systeme erzeugen Inhalte, die neu erscheinen, ohne originär im kantischen Sinne zu sein. Ihre Leistungen basieren auf der Rekombination vorhandener Daten, nicht auf autonomer Regelsetzung. Originalität verschiebt sich dadurch von einem ontologischen zu einem funktionalen Kriterium: Neu ist, was als neu wahrgenommen wird.\par

Hier bietet der Utilitarismus von John Stuart Mill einen ergänzenden Zugang. Mill bewertet Handlungen nicht nach ihrer inneren Motivation, sondern nach ihren Folgen. Übertragen auf kreative Werke bedeutet dies, dass der moralische und rechtliche Status eines Outputs weniger von seiner Entstehung als von seinem gesellschaftlichen Nutzen abhängt. Ein Werk ist relevant, wenn es zur Förderung von Wissen, Wohlstand oder kultureller Entwicklung beiträgt.\par

Diese Perspektive ist besonders für Fragen des geistigen Eigentums relevant. Während Kant Autorschaft als Ausdruck personaler Freiheit begreift, erlaubt Mill eine stärker pragmatische Betrachtung: Schutzrechte können gerechtfertigt sein, wenn sie insgesamt positive Folgen haben, etwa durch Anreize für kreative Produktion. Im Fall von KI stellt sich daher weniger die Frage, ob sie originell „schafft“, sondern ob und wie ihre Nutzung reguliert werden sollte, um gesellschaftlichen Nutzen zu maximieren, ohne menschliche Urheberrechte zu untergraben.\par

Originalität, Inspiration und geistiges Eigentum erscheinen somit nicht als feste Eigenschaften eines Werkes, sondern als normative Zuschreibungen, die im Spannungsfeld zwischen individueller Autorschaft, kollektiver Kreativität und gesellschaftlichem Nutzen neu ausgehandelt werden müssen.\par

\subsection{Funktionsweise generativer KI}
Generative KI bezeichnet Systeme, die auf Basis erlernter statistischer Muster neue Inhalte erzeugen können, etwa Texte, Bilder oder Audio \cite{goodfellow2016_deepLearning}. Für Large Language Models (LLMs) besteht diese Generativität im Kern darin, aus einem gegebenen Kontext Wahrscheinlichkeitsverteilungen über mögliche Fortsetzungen zu berechnen und daraus Token-sequenzen zu generieren. Der „Output“ ist damit nicht das Ergebnis eines Verständnisses im eigentlichem Sinne, sondern einer modellierten sprachlichen Regularität, die aus Trainingsdaten abstrahiert wurde.\par

\subsubsection{Prinzipien von Machine Learning und Large Language Models}
Machine Learning (ML) bezeichnet Verfahren, bei denen ein Modell aus Beispieldaten eine Abbildungsfunktion lernt, ohne dass alle Regeln explizit programmiert werden \cite{bishop2006pattern}. Beim überwachten Lernen werden Eingaben und Zielausgaben (Labels) vorgegeben; beim unüberwachten Lernen werden Strukturen (z.\,B. Cluster) in Daten identifiziert; bei bestärkendem Lernen (Reinforcement Learning) werden Strategien über Rückmeldesignale optimiert. LLMs werden primär über (selbst-)überwachtes Lernen trainiert: Sie erhalten Textsequenzen und lernen, das nächste Token vorherzusagen\cite{goodfellow2016_deepLearning}.\par

Moderne LLMs beruhen typischerweise auf der Transformer-Architektur. Zentrale Bausteine sind Tokenisierung (Zerlegung von Text in modellierbare Einheiten), Einbettungen (Embeddings) als Vektorrepräsentationen sowie das Attention-Prinzip, das es dem Modell erlaubt, relevante Kontextteile bei der Vorhersage zu gewichten\cite{vaswani2023attentionneed}. Training und Inferenz unterscheiden sich dabei deutlich: Während im Training Parameter durch Optimierung (Gradientenabstieg und Backpropagation) angepasst werden, werden in der Anwendung (Inferenz) aus den fixierten Parametern Wahrscheinlichkeiten berechnet und per Decoding-Strategie (z.\,B. greedy, beam search, sampling) konkrete Ausgaben erzeugt\cite{goodfellow2016_deepLearning}.\par

Viele Systeme werden zusätzlich anwendungsnah angepasst, etwa durch Fine-Tuning auf domänenspezifischen Daten oder durch Verfahren wie RLHF (Reinforcement Learning from Human Feedback), bei denen menschliche Präferenzen genutzt werden, um Ausgabequalität und Sicherheitsverhalten zu steuern\cite{ouyang2022training}. Diese Schritte verändern jedoch nicht den Grundmechanismus: die wahrscheinlichkeitsbasierte Sequenzfortsetzung auf Basis erlernter Regularitäten.\par

\subsubsection{Trainingsdaten, neuronale Netze und Mustererkennung}
Die Leistungsfähigkeit generativer Modelle hängt stark von Umfang, Vielfalt und Qualität der Trainingsdaten ab. Trainingssätze bestehen typischerweise aus großen Mengen öffentlich zugänglicher Texte (und je nach System zusätzlich aus Code, Dialogdaten oder multimodalen Daten). Aus ihnen lernt das Modell statistische Zusammenhänge auf verschiedenen Ebenen: Orthographie und Grammatik, semantische Assoziationen, Stilmerkmale sowie konventionalisierte Argumentations- und Textmuster.\par

Neuronale Netze implementieren diese Lernprozesse als Schichtung vieler parametrischer Operationen. Während der Trainingsphase werden Parameter so angepasst, dass die Vorhersagefehler über viele Beispiele minimiert werden. Dadurch entsteht eine Form der Mustererkennung, die nicht auf expliziten Regeln basiert, sondern auf „verteilten“ Repräsentationen\cite{goodfellow2016_deepLearning}: Bedeutung wird nicht als feste Symbolstruktur codiert, sondern als relationales Muster im Parameterraum.\par

Gleichzeitig erklärt dieser Mechanismus typische Eigenschaften generativer KI: Sie kann sehr kohärente Texte in bekannten Genres erzeugen, weil entsprechende Regularitäten in den Daten häufig vorkommen; sie kann aber auch plausibel klingende, dennoch falsche Aussagen erzeugen, wenn die wahrscheinlichste Fortsetzung nicht mit der Wahrheit übereinstimmt. Der Output ist somit nicht per se verlässlich, sondern spiegelt statistische Plausibilität wider.\par

\subsubsection{Grenzen maschineller Kreativität: Reproduktion statt Intentionalität}
Im Anschluss an die zuvor diskutierten Autorschaftskonzepte lässt sich eine zentrale Grenze generativer KI so fassen: Ihre „Kreativität“ ist im Wesentlichen funktional und nicht intentional. Das Modell verfolgt keine eigenen Zwecke, hat kein Selbstverhältnis und keine Verantwortungsfähigkeit; es optimiert lediglich eine formale Zielgröße (Vorhersagegüte bzw. Präferenzmodelle) und erzeugt daraus einen Output. Neuheit entsteht dabei häufig als Rekombination und Variation bestehender Muster, nicht als Ausdruck einer erfahrungsbasierten Perspektive oder eines reflektierten Gestaltungswillens\cite{Boden_MindsAndMachines_2009}.\par

Diese Differenz zeigt sich besonders deutlich in Kontexten, in denen Autorschaft mit Zurechenbarkeit verknüpft ist: Während menschliche Autoren Gründe angeben, Absichten vertreten und Verantwortung übernehmen können, kann ein Modell dies nur simulieren, indem es sprachliche Muster von Begründungen reproduziert. Auch wenn LLM-Ausgaben für Rezipienten originell wirken können, folgt daraus weder ein innerer Sinnhorizont noch ein normativ zurechenbares Handeln.\par

Für die weitere Arbeit ist daher leitend: Generative KI kann produktiv an kreativen Prozessen beteiligt sein und neue Kombinationen im Ergebnis hervorbringen; sie bleibt jedoch ein Werkzeug, dessen Output aus statistischer Musterfortsetzung hervorgeht. Die Autorschaftsfrage verschiebt sich damit von der Idee eines „schöpferischen Subjekts“ hin zu Praktiken der Nutzung, Steuerung und Verantwortung in menschlich-technischen Konstellationen.\par